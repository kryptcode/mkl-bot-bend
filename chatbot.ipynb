{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854acb67",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 162603: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# Import pandas for data processing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/koye/Documents/GitHub/mkl-bot-bend/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m your_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(your_dataset\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\koye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\koye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\koye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\koye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 162603: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import pandas for data processing\n",
    "file_path = 'C:/Users/koye/Documents/GitHub/mkl-bot-bend/train.csv'\n",
    "your_dataset = pd.read_csv(file_path)\n",
    "# Display the first few rows of the DataFrame\n",
    "print(your_dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27f3a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Context  \\\n",
      "3507  My grandson's step-mother sends him to school ...   \n",
      "3508  My boyfriend is in recovery from drug addictio...   \n",
      "3509  The birth mother attempted suicide several tim...   \n",
      "3510  I think adult life is making him depressed and...   \n",
      "3511  I just took a job that requires me to travel f...   \n",
      "\n",
      "                                               Response  \n",
      "3507  Absolutely not! It is never in a child's best ...  \n",
      "3508  I'm sorry you have tension between you and you...  \n",
      "3509  The true answer is, \"no one can really say wit...  \n",
      "3510  How do you help yourself to believe you requir...  \n",
      "3511                           hmm this is a tough one!  \n"
     ]
    }
   ],
   "source": [
    "print(your_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202010c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context     0\n",
       "Response    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5800d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Context Response\n",
      "2434  From the moment I wake up, I hear what I think...      NaN\n",
      "3007  I’m trying to make marriage work after a split...      NaN\n",
      "3224  Every winter I find myself getting sad because...      NaN\n",
      "3225  Does counseling really do anything that can he...      NaN\n"
     ]
    }
   ],
   "source": [
    "rows_with_null_response = your_dataset[your_dataset['Response'].isnull()]\n",
    "print(rows_with_null_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3df89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (3.7)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "     --------------------------------------- 12.1/12.1 MB 52.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: transformers in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "     ------------------------------------- 122.2/122.2 kB 28.4 kB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "     --------------------------------------- 46.0/46.0 kB 28.2 kB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "     ------------------------------------- 481.9/481.9 kB 40.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 81.5 kB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "     ------------------------------------ 182.0/182.0 kB 215.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "     -------------------------------------- 50.1/50.1 kB 116.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "     ------------------------------------ 409.3/409.3 kB 122.8 kB/s eta 0:00:00\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "     ---------------------------------------- 5.4/5.4 MB 188.0 kB/s eta 0:00:00\n",
      "Collecting pydantic-core==2.18.2\n",
      "  Downloading pydantic_core-2.18.2-cp310-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 110.8 kB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 190.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.0/45.0 kB 445.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alabi abigail\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.1.1-cp310-cp310-win_amd64.whl (152 kB)\n",
      "     ------------------------------------ 152.7/152.7 kB 569.9 kB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, language-data, cloudpathlib, pydantic, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.1 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.1 pydantic-core-2.18.2 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 typing-extensions-4.11.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk spacy transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7186382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ALABI\n",
      "[nltk_data]     ABIGAIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Context  \\\n",
      "0     I'm going through some things with my feelings...   \n",
      "1     I'm going through some things with my feelings...   \n",
      "2     I'm going through some things with my feelings...   \n",
      "3     I'm going through some things with my feelings...   \n",
      "4     I'm going through some things with my feelings...   \n",
      "...                                                 ...   \n",
      "3507  My grandson's step-mother sends him to school ...   \n",
      "3508  My boyfriend is in recovery from drug addictio...   \n",
      "3509  The birth mother attempted suicide several tim...   \n",
      "3510  I think adult life is making him depressed and...   \n",
      "3511  I just took a job that requires me to travel f...   \n",
      "\n",
      "                                               Response  \\\n",
      "0     If everyone thinks you're worthless, then mayb...   \n",
      "1     Hello, and thank you for your question and see...   \n",
      "2     First thing I'd suggest is getting the sleep y...   \n",
      "3     Therapy is essential for those that are feelin...   \n",
      "4     I first want to let you know that you are not ...   \n",
      "...                                                 ...   \n",
      "3507  Absolutely not! It is never in a child's best ...   \n",
      "3508  I'm sorry you have tension between you and you...   \n",
      "3509  The true answer is, \"no one can really say wit...   \n",
      "3510  How do you help yourself to believe you requir...   \n",
      "3511                           hmm this is a tough one!   \n",
      "\n",
      "                                         Context_Tokens  \\\n",
      "0     [I, 'm, going, through, some, things, with, my...   \n",
      "1     [I, 'm, going, through, some, things, with, my...   \n",
      "2     [I, 'm, going, through, some, things, with, my...   \n",
      "3     [I, 'm, going, through, some, things, with, my...   \n",
      "4     [I, 'm, going, through, some, things, with, my...   \n",
      "...                                                 ...   \n",
      "3507  [My, grandson, 's, step-mother, sends, him, to...   \n",
      "3508  [My, boyfriend, is, in, recovery, from, drug, ...   \n",
      "3509  [The, birth, mother, attempted, suicide, sever...   \n",
      "3510  [I, think, adult, life, is, making, him, depre...   \n",
      "3511  [I, just, took, a, job, that, requires, me, to...   \n",
      "\n",
      "                                        Response_Tokens  \n",
      "0     [If, everyone, thinks, you, 're, worthless, ,,...  \n",
      "1     [Hello, ,, and, thank, you, for, your, questio...  \n",
      "2     [First, thing, I, 'd, suggest, is, getting, th...  \n",
      "3     [Therapy, is, essential, for, those, that, are...  \n",
      "4     [I, first, want, to, let, you, know, that, you...  \n",
      "...                                                 ...  \n",
      "3507  [Absolutely, not, !, It, is, never, in, a, chi...  \n",
      "3508  [I, 'm, sorry, you, have, tension, between, yo...  \n",
      "3509  [The, true, answer, is, ,, ``, no, one, can, r...  \n",
      "3510  [How, do, you, help, yourself, to, believe, yo...  \n",
      "3511                  [hmm, this, is, a, tough, one, !]  \n",
      "\n",
      "[3512 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your dataset from the file\n",
    "df = pd.read_csv('C:/Users/koye/Documents/GitHub/mkl-bot-bend/train.csv')  # Replace 'your_dataset.csv' with your actual file path\n",
    "\n",
    "# Tokenize function using NLTK word_tokenize\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text) if pd.notnull(text) else []\n",
    "\n",
    "# Tokenize the text in specified columns\n",
    "df['Context_Tokens'] = df['Context'].apply(tokenize_text)\n",
    "df['Response_Tokens'] = df['Response'].apply(tokenize_text)\n",
    "\n",
    "# Display the DataFrame with tokenized columns\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722f4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ALABI\n",
      "[nltk_data]     ABIGAIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original BoW Vector Shape:\n",
      "(3512, 15116)\n",
      "\n",
      "Padded BoW Vector Shape:\n",
      "(3512, 15116)\n",
      "\n",
      "Example of Padded BoW Vector:\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your dataset from the file\n",
    "df = pd.read_csv('C:/Users/koye/Documents/GitHub/mkl-bot-bend/train.csv')  # Replace with your actual file path\n",
    "\n",
    "# Tokenize function using NLTK word_tokenize\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text) if pd.notnull(text) else []\n",
    "\n",
    "# Tokenize the text in specified columns\n",
    "df['Context_Tokens'] = df['Context'].apply(tokenize_text)\n",
    "df['Response_Tokens'] = df['Response'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized text to sentences\n",
    "df['Context_Sentences'] = df['Context_Tokens'].apply(lambda x: ' '.join(x))\n",
    "df['Response_Sentences'] = df['Response_Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize CountVectorizer for BoW encoding\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the entire dataset\n",
    "vectorizer.fit(df['Context_Sentences'].tolist() + df['Response_Sentences'].tolist())\n",
    "\n",
    "# Transform the tokenized text into BoW vectors\n",
    "X_context = vectorizer.transform(df['Context_Sentences']).toarray()\n",
    "X_response = vectorizer.transform(df['Response_Sentences']).toarray()\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = max(X_context.shape[1], X_response.shape[1])\n",
    "\n",
    "X_context_padded = pad_sequences(X_context, maxlen=max_sequence_length, padding='post')\n",
    "X_response_padded = pad_sequences(X_response, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Example of sequence encoding\n",
    "print(\"Original BoW Vector Shape:\")\n",
    "print(X_context.shape)\n",
    "\n",
    "print(\"\\nPadded BoW Vector Shape:\")\n",
    "print(X_context_padded.shape)\n",
    "\n",
    "print(\"\\nExample of Padded BoW Vector:\")\n",
    "print(X_context_padded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a377e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ALABI\n",
      "[nltk_data]     ABIGAIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Context  \\\n",
      "0     I'm going through some things with my feelings...   \n",
      "1     I'm going through some things with my feelings...   \n",
      "2     I'm going through some things with my feelings...   \n",
      "3     I'm going through some things with my feelings...   \n",
      "4     I'm going through some things with my feelings...   \n",
      "...                                                 ...   \n",
      "3507  My grandson's step-mother sends him to school ...   \n",
      "3508  My boyfriend is in recovery from drug addictio...   \n",
      "3509  The birth mother attempted suicide several tim...   \n",
      "3510  I think adult life is making him depressed and...   \n",
      "3511  I just took a job that requires me to travel f...   \n",
      "\n",
      "                                               Response  \\\n",
      "0     If everyone thinks you're worthless, then mayb...   \n",
      "1     Hello, and thank you for your question and see...   \n",
      "2     First thing I'd suggest is getting the sleep y...   \n",
      "3     Therapy is essential for those that are feelin...   \n",
      "4     I first want to let you know that you are not ...   \n",
      "...                                                 ...   \n",
      "3507  Absolutely not! It is never in a child's best ...   \n",
      "3508  I'm sorry you have tension between you and you...   \n",
      "3509  The true answer is, \"no one can really say wit...   \n",
      "3510  How do you help yourself to believe you requir...   \n",
      "3511                           hmm this is a tough one!   \n",
      "\n",
      "                                     Context_BoW_Padded  \\\n",
      "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "...                                                 ...   \n",
      "3507  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3508  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3509  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3510  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3511  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                    Response_BoW_Padded  \n",
      "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                 ...  \n",
      "3507  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3508  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3509  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3510  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3511  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[3512 rows x 4 columns]\n",
      "                                             Context  \\\n",
      "0  I'm going through some things with my feelings...   \n",
      "1  I'm going through some things with my feelings...   \n",
      "2  I'm going through some things with my feelings...   \n",
      "3  I'm going through some things with my feelings...   \n",
      "4  I'm going through some things with my feelings...   \n",
      "\n",
      "                                            Response  \\\n",
      "0  If everyone thinks you're worthless, then mayb...   \n",
      "1  Hello, and thank you for your question and see...   \n",
      "2  First thing I'd suggest is getting the sleep y...   \n",
      "3  Therapy is essential for those that are feelin...   \n",
      "4  I first want to let you know that you are not ...   \n",
      "\n",
      "                                  Context_BoW_Padded  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                 Response_BoW_Padded  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your dataset from the file\n",
    "df = pd.read_csv('C:/Users/koye/Documents/GitHub/mkl-bot-bend/train.csv')  # Replace with your actual file path\n",
    "\n",
    "# Tokenize function using NLTK word_tokenize\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text) if pd.notnull(text) else []\n",
    "\n",
    "# Tokenize the text in specified columns\n",
    "df['Context_Tokens'] = df['Context'].apply(tokenize_text)\n",
    "df['Response_Tokens'] = df['Response'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized text to sentences\n",
    "df['Context_Sentences'] = df['Context_Tokens'].apply(lambda x: ' '.join(x))\n",
    "df['Response_Sentences'] = df['Response_Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize CountVectorizer for BoW encoding\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the entire dataset\n",
    "vectorizer.fit(df['Context_Sentences'].tolist() + df['Response_Sentences'].tolist())\n",
    "\n",
    "# Transform the tokenized text into BoW vectors\n",
    "X_context = vectorizer.transform(df['Context_Sentences']).toarray()\n",
    "X_response = vectorizer.transform(df['Response_Sentences']).toarray()\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = max(X_context.shape[1], X_response.shape[1])\n",
    "\n",
    "X_context_padded = pad_sequences(X_context, maxlen=max_sequence_length, padding='post')\n",
    "X_response_padded = pad_sequences(X_response, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Add the padded vectors to the DataFrame for inspection\n",
    "df['Context_BoW_Padded'] = list(X_context_padded)\n",
    "df['Response_BoW_Padded'] = list(X_response_padded)\n",
    "\n",
    "# Print the DataFrame with the padded BoW vectors\n",
    "print(df[['Context', 'Response', 'Context_BoW_Padded', 'Response_BoW_Padded']])\n",
    "\n",
    "# Optionally, print a subset for clearer view (e.g., first 5 rows)\n",
    "print(df[['Context', 'Response', 'Context_BoW_Padded', 'Response_BoW_Padded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba332d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.21652421652421652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('C:/Users/koye/Documents/GitHub/mkl-bot-bend/train.csv')\n",
    "df.dropna(inplace=True)  # Drop rows with missing values\n",
    "\n",
    "# Tokenization and Vectorization\n",
    "df['Context_Tokens'] = df['Context'].apply(nltk.word_tokenize)\n",
    "df['Response_Tokens'] = df['Response'].apply(nltk.word_tokenize)\n",
    "df['Context_Sentences'] = df['Context_Tokens'].apply(lambda x: ' '.join(x))\n",
    "df['Response_Sentences'] = df['Response_Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Context_Sentences'])\n",
    "y = df['Response_Sentences']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ALABI\n",
      "[nltk_data]     ABIGAIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2170212765957447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ALABI ABIGAIL\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hi\n",
      "Bot: Hello , How are you today\n",
      "You: I am not fine\n",
      "Bot: Tell me about it\n",
      "You: i have bad dreams\n",
      "Bot: Really , what are they always about\n",
      "You: about stress\n",
      "Bot: How tell me about it .\n",
      "You: i go through a lot at work\n",
      "Bot: How tell me about it .\n",
      "You: I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.    I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.    How can I change my feeling of being worthless to everyone?\n",
      "Bot: Heck , sure thing , hun ! Feelings of 'depression ' have a deeply-rooted base in physical structures that may not be functioning very well at present ; and , we can certainly turn them on again using means that you are able to find around the house and with relative ease : ) After that , emotional and spiritual support will be liberally applied .\n",
      "You: thank you\n",
      "Bot: You are always welcome\n",
      "You: bye\n",
      "Bot: Have a nice day and know I am always here to help\n",
      "You: ok\n",
      "Bot: As a depth therapist ( aka `` psychodynamic practitioner '' ) , I do a lot of dream work with clients ; and they gain great insights from our explorations.We remember dreams , I believe , because dreams are messages ( always in code ) from the unconscious , communicating the crux of unresolved conflict between who we are and who we think we are supposed to be . You might be remembering so many dreams because your conscious mind is ready to hear these messages ... and make some internal changes in response.Write them down ! You 'll be glad you did . A broad cross-section of these these messages will help your therapist help you interpret the meanings of these communications from you to you , and possibly chart your best path forward toward to resolve the cause of psychological symptoms you might be experiencing .\n",
      "You: I have so many issues to address. I have a history of sexual abuse, Iâ€™m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and Iâ€™m beginning to have anxiety. I have low self esteem but Iâ€™ve been happily married for almost 35 years.    Iâ€™ve never had counseling about any of this. Do I have too many issues to address in counseling?\n",
      "Bot: It is very common for people to have multiple issues that they want to ( and need to ) address in counseling . I have had clients ask that same question and through more exploration , there is often an underlying fear that they `` ca n't be helped '' or that they will `` be too much for their therapist . '' I do n't know if any of this rings true for you . But , most people have more than one problem in their lives and more often than not , people have numerous significant stressors in their lives . Let 's face it , life can be complicated ! Therapists are completely ready and equipped to handle all of the issues small or large that a client presents in session . Most therapists over the first couple of sessions will help you prioritize the issues you are facing so that you start addressing the issues that are causing you the most distress . You can never have too many issues to address in counseling . All of the issues you mention above can be successfully worked through in counseling .\n",
      "You: Hi\n",
      "Bot: Hello , How are you today\n",
      "You: I am fine\n",
      "Bot: Tell me about it\n",
      "You: about stress\n",
      "Bot: How tell me about it .\n",
      "You: I have so many issues to address. I have a history of sexual abuse, Iâ€™m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and Iâ€™m beginning to have anxiety. I have low self esteem but Iâ€™ve been happily married for almost 35 years.    Iâ€™ve never had counseling about any of this. Do I have too many issues to address in counseling?\n",
      "Bot: It is very common for people to have multiple issues that they want to ( and need to ) address in counseling . I have had clients ask that same question and through more exploration , there is often an underlying fear that they `` ca n't be helped '' or that they will `` be too much for their therapist . '' I do n't know if any of this rings true for you . But , most people have more than one problem in their lives and more often than not , people have numerous significant stressors in their lives . Let 's face it , life can be complicated ! Therapists are completely ready and equipped to handle all of the issues small or large that a client presents in session . Most therapists over the first couple of sessions will help you prioritize the issues you are facing so that you start addressing the issues that are causing you the most distress . You can never have too many issues to address in counseling . All of the issues you mention above can be successfully worked through in counseling .\n",
      "You: I am pregnant\n",
      "Bot: What a wonderful question ! Good for you on clearly knowing your priorities.If I was sitting with you now , I 'd ask you to list your reasons.Start with whatever answers you do come up with and examine each of these a little further.If , for example , you feel you need a man because many of your friends are in relationships , then possibly you feel insecure about being your unique self , even when this sets you apart from your friends.If , you feel you need a man to protect you financially , then possibly you 've lost faith in your ability to financially support yourself.If you 'd like a man in your life to offer your love and are willing to contribute the work of relating intimately , then you 've found the best reason for wanting a man in your life.This is far different than `` need '' .\n",
      "You: Ekaro\n",
      "Bot: As a depth therapist ( aka `` psychodynamic practitioner '' ) , I do a lot of dream work with clients ; and they gain great insights from our explorations.We remember dreams , I believe , because dreams are messages ( always in code ) from the unconscious , communicating the crux of unresolved conflict between who we are and who we think we are supposed to be . You might be remembering so many dreams because your conscious mind is ready to hear these messages ... and make some internal changes in response.Write them down ! You 'll be glad you did . A broad cross-section of these these messages will help your therapist help you interpret the meanings of these communications from you to you , and possibly chart your best path forward toward to resolve the cause of psychological symptoms you might be experiencing .\n",
      "You: bye\n",
      "Bot: Have a nice day and know I am always here to help\n",
      "You: thank you\n",
      "Bot: You are always welcome\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('C:/Users/koye/Documents/GitHub/mkl-bot-bend/train.csv')\n",
    "df.dropna(inplace=True)  # Drop rows with missing values\n",
    "\n",
    "# Tokenization and Vectorization\n",
    "df['Context_Tokens'] = df['Context'].apply(nltk.word_tokenize)\n",
    "df['Response_Tokens'] = df['Response'].apply(nltk.word_tokenize)\n",
    "df['Context_Sentences'] = df['Context_Tokens'].apply(lambda x: ' '.join(x))\n",
    "df['Response_Sentences'] = df['Response_Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Context_Sentences'])\n",
    "y = df['Response_Sentences']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to get response for a given input\n",
    "def get_response(user_input):\n",
    "    user_input_tokenized = ' '.join(word_tokenize(user_input))\n",
    "    user_input_vectorized = vectorizer.transform([user_input_tokenized])\n",
    "    response = log_reg.predict(user_input_vectorized)\n",
    "    return response[0]\n",
    "\n",
    "# Get input from user and provide a response\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d77f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.10.4)\n",
      "Requirement already satisfied: nbclient in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat) (2.19.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat) (4.22.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbclient) (8.6.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient) (6.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (306)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\koye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbformat nbclient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
